{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 5 GPT Project IS 640"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the group 5 GPT Project for IS 640 - Programming for Business Analytics\n",
    "\n",
    "Members:\n",
    "- Hans \n",
    "- Chetan  \n",
    "- Danish \n",
    "- Srujana \n",
    "- Bruna "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 1: Dataset Exploration and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all the modules and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description: \n",
    "This dataset contains information about TV series from IMDb, including details such as title, IMDb ID, release year, genre, cast, synopsis, rating, runtime, certificate, number of votes, and gross revenue. The data is scraped from the IMDb website using web scraping techniques and is organized into separate CSV files for each genre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features:\n",
    "\n",
    "- Title: The title of the TV series.\n",
    "- IMDb ID: The unique identifier for the series on IMDb.\n",
    "- Release Year: The year in which the series was released.\n",
    "- Genre: The genre(s) of the series.\n",
    "- Cast: The main cast members of the series.\n",
    "- Synopsis: A brief summary or description of the series.\n",
    "- Rating: The average rating of the series on IMDb (scaled from 1 to 10).\n",
    "- Runtime: The duration of each episode or the total runtime of the series.\n",
    "- Certificate: The content rating or certificate assigned to the series (e.g., PG-13, TV-MA).\n",
    "- Number of Votes: The total number of votes or ratings received by the series.\n",
    "- Gross Revenue: The total gross revenue generated by the series (if available)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective:\n",
    "\n",
    "We aim to generate text using the GPT transformer model, focusing exclusively on the 'Synopsis' column of the TV series dataset. Our goal is to clean and preprocess the 'Synopsis' data by converting all text to lowercase and replacing non-alphanumeric characters (except dots) with spaces, and then utilize the GPT transformer to generate coherent and relevant text based on the cleaned synopsis data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the Zip folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'tv_series_data.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Extract the ZIP file to a folder\u001b[39;00m\n\u001b[1;32m      5\u001b[0m extracted_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtv_series_data\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mzipfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzip_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m zip_ref:\n\u001b[1;32m      7\u001b[0m     zip_ref\u001b[38;5;241m.\u001b[39mextractall(extracted_folder)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.8/zipfile.py:1253\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1251\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1252\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1253\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilemode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1254\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   1255\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filemode \u001b[38;5;129;01min\u001b[39;00m modeDict:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tv_series_data.zip'"
     ]
    }
   ],
   "source": [
    "# Path to the local ZIP file\n",
    "zip_file_path = 'tv_series_data.zip'\n",
    "\n",
    "# Extract the ZIP file to a folder\n",
    "extracted_folder = 'tv_series_data'\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extracted_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all CSV files into one DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "combined_data = pd.DataFrame()\n",
    "for file in os.listdir(extracted_folder):\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(extracted_folder, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        combined_data = pd.concat([combined_data, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the first 5 rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the combined data and view the information of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Data Shape: (236828, 11)\n",
      "\n",
      "Combined Data Columns: Index(['Title', 'IMDb ID', 'Release Year', 'Genre', 'Cast', 'Synopsis',\n",
      "       'Rating', 'Runtime', 'Certificate', 'Number of Votes', 'Gross Revenue'],\n",
      "      dtype='object')\n",
      "\n",
      "Combined Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 236828 entries, 0 to 236827\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count   Dtype  \n",
      "---  ------           --------------   -----  \n",
      " 0   Title            236828 non-null  object \n",
      " 1   IMDb ID          236828 non-null  object \n",
      " 2   Release Year     236819 non-null  object \n",
      " 3   Genre            236828 non-null  object \n",
      " 4   Cast             235956 non-null  object \n",
      " 5   Synopsis         236828 non-null  object \n",
      " 6   Rating           236828 non-null  float64\n",
      " 7   Runtime          216983 non-null  object \n",
      " 8   Certificate      169091 non-null  object \n",
      " 9   Number of Votes  236828 non-null  object \n",
      " 10  Gross Revenue    45611 non-null   object \n",
      "dtypes: float64(1), object(10)\n",
      "memory usage: 19.9+ MB\n",
      "None\n",
      "\n",
      "Combined Data Description:\n",
      "              Rating\n",
      "count  236828.000000\n",
      "mean        6.699968\n",
      "std         1.338342\n",
      "min         1.000000\n",
      "25%         6.000000\n",
      "50%         6.800000\n",
      "75%         7.600000\n",
      "max        10.000000\n",
      "\n",
      "Missing Values:\n",
      "Title                   0\n",
      "IMDb ID                 0\n",
      "Release Year            9\n",
      "Genre                   0\n",
      "Cast                  872\n",
      "Synopsis                0\n",
      "Rating                  0\n",
      "Runtime             19845\n",
      "Certificate         67737\n",
      "Number of Votes         0\n",
      "Gross Revenue      191217\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Combined Data Shape:\", combined_data.shape)\n",
    "print(\"\\nCombined Data Columns:\", combined_data.columns)\n",
    "print(\"\\nCombined Data Info:\")\n",
    "print(combined_data.info())\n",
    "print(\"\\nCombined Data Description:\")\n",
    "print(combined_data.describe())\n",
    "print(\"\\nMissing Values:\")\n",
    "print(combined_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the data: convert to lowercase and replace non-alphanumeric characters (except dots) with spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    return ''.join(char.lower() if char.isalnum() or char == '.' else ' ' for char in text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new df called cleaned_data which contains only the cleaned text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = combined_data['Synopsis'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = pd.DataFrame(cleaned_text, columns=['Synopsis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cleaned_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Display the first few rows of the cleaned data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcleaned_data\u001b[49m\u001b[38;5;241m.\u001b[39mhead())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cleaned_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the cleaned data\n",
    "print(cleaned_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename column header from Synopsis to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = cleaned_data.rename(columns={'Synopsis': 'text'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save it into a csv file called `tv_series_synopsis_full.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned data to a CSV file\n",
    "cleaned_data.to_csv('tv_series_synopsis_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the hyperparameters for fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10f6a0cf0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32 # Number of sequences processed in parallel during training\n",
    "block_size = 128 # Maximum context length for predictions (sequence length)\n",
    "max_iters = 5000 # Total number of training iterations\n",
    "eval_interval = 100 # How often to evaluate the model (every 100 iterations)\n",
    "learning_rate = 1e-3  # Step size for gradient descent optimization\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # Use GPU if available, otherwise CPU\n",
    "eval_iters = 200 # Number of iterations for loss estimation during evaluation\n",
    "n_embd = 128 # Dimensionality of the token embeddings and model's hidden layers\n",
    "n_head = 8  # Number of attention heads in each self-attention layer\n",
    "n_layer = 8 # Number of transformer layers in the model\n",
    "dropout = 0.1 # Probability of dropping out neurons during training (regularization)\n",
    "\n",
    "torch.manual_seed(1337)  # Set random seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing TV show data as the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'miles morales catapults across the multiverse, where he encounters a team of spiderpeople charged with protecting its very existence. when the heroes clash on how to handle a new threat, miles must redefine what it means to be a hero. a c.i.a. operative on the edge of retirement discovers a family secret and is called back into the field for one last job. a hit man from the midwest moves to los angeles and gets caught up in the citys theatre arts scene. john wick uncovers a path to defeating the'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Dataset/tv_series_synopsis_full.csv', encoding='latin-1')\n",
    "df['combined'] =  df['text'].astype(str)\n",
    "text = \" \".join(df['combined'].dropna().tolist())\n",
    "text[:500]  # print the first 500 characters of the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting string to numerical format for training and testing.\n",
    "1. Extract the unique characters and find the count of the vocabulary\n",
    "2. Map the characters to integers and vice versa\n",
    "3. Define the encode function which converts strings into numerical format\n",
    "4. Define the decode function which converts numbers into strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diving the data into training and validation sets\n",
    "1. Encode the text into numbers so that it can be processed as a pytorch tensor\n",
    "2. Define the split ratio\n",
    "3. Make the training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create functions for batch loading and loss estimation\n",
    "`get_batch`:\n",
    "Creates small, random batches of input-output pairs for training or validation.\n",
    "Ensures the model learns from diverse examples within the dataset.\n",
    "\n",
    "`estimate_loss`:\n",
    "Provides a measure of the model's performance on both training and validation datasets.\n",
    "Helps monitor overfitting (training loss much lower than validation loss) and guide hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    \"\"\"\n",
    "    Generate a small batch of data of inputs x and targets y.\n",
    "\n",
    "    Args:\n",
    "        split: 'train' or 'val'. if 'train', we sample from train_data, otherwise val_data\n",
    "\n",
    "    Returns:\n",
    "        x: a tensor of shape (bs, block_size) representing the input sequence\n",
    "        y: a tensor of shape (bs, block_size) representing the target sequence\n",
    "    \"\"\"\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    \"\"\"\n",
    "    Estimates the average loss for the training and validation datasets \n",
    "    over a fixed number of evaluation iterations.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: A dictionary containing the mean loss for both the \n",
    "        training and validation datasets. Keys are:\n",
    "            - 'train': Mean loss for the training dataset.\n",
    "            - 'val': Mean loss for the validation dataset.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 2: Basic Model Usage (Bigram Language Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: This milestone introduces a simple bigram language model. It predicts the next token based solely on the current token, without considering any broader context.\n",
    "\n",
    "How it works: The model uses a simple lookup table to predict the next token based on the current one.\n",
    "\n",
    "Code changes:\n",
    "- Implementation of a basic nn.Embedding layer for token prediction\n",
    "- Simple forward pass that uses only the current token to predict the next\n",
    "\n",
    "Metrics: Basic tracking of training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple bigram-based language model that predicts the next token \n",
    "    based on the current token using an embedding layer. This model is \n",
    "    primarily used as a basic demonstration of language modeling concepts.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): The size of the vocabulary, defining the number of unique tokens.\n",
    "\n",
    "    Attributes:\n",
    "        token_embedding_table (nn.Embedding): Embedding layer that maps tokens to logits \n",
    "            for all tokens in the vocabulary.\n",
    "\n",
    "    Methods:\n",
    "        forward(idx, targets=None):\n",
    "            Performs the forward pass of the model, computing logits for the next token \n",
    "            and optionally calculating the cross-entropy loss.\n",
    "\n",
    "            Args:\n",
    "                idx (torch.Tensor): Tensor of shape (B, T) containing input token indices, \n",
    "                    where B is the batch size and T is the sequence length.\n",
    "                targets (torch.Tensor, optional): Tensor of shape (B, T) containing target \n",
    "                    token indices for loss computation. Default is None.\n",
    "\n",
    "            Returns:\n",
    "                Tuple[torch.Tensor, torch.Tensor or None]:\n",
    "                    - logits (torch.Tensor): Tensor of shape (B, T, vocab_size) containing \n",
    "                      predicted logits for the next token.\n",
    "                    - loss (torch.Tensor or None): Scalar tensor representing the cross-entropy \n",
    "                      loss if `targets` is provided, otherwise None.\n",
    "\n",
    "        generate(idx, max_new_tokens):\n",
    "            Generates a sequence of tokens by sampling from the model's predictions.\n",
    "\n",
    "            Args:\n",
    "                idx (torch.Tensor): Tensor of shape (B, T) containing the initial context \n",
    "                    (sequence of token indices).\n",
    "                max_new_tokens (int): Number of new tokens to generate.\n",
    "\n",
    "            Returns:\n",
    "                torch.Tensor: Tensor of shape (B, T + max_new_tokens) containing the initial \n",
    "                context concatenated with the generated tokens.\n",
    "\n",
    "    Examples:\n",
    "        >>> vocab_size = 100\n",
    "        >>> model = BigramLanguageModel(vocab_size)\n",
    "        >>> idx = torch.tensor([[1, 2, 3]])\n",
    "        >>> logits, loss = model(idx, targets=torch.tensor([[2, 3, 4]]))\n",
    "        >>> generated_sequence = model.generate(idx, max_new_tokens=5)\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001681 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a PyTorch optimizer for updating the model's parameter's during training\n",
    "AdamW is a variant of the Adam optimizer that includes decoupled weight decay, making it better suited for modern deep learning models like transformers.\n",
    "Key features:\n",
    "Combines adaptive learning rates (like Adam) with the L2 regularization benefits of weight decay.\n",
    "Helps prevent overfitting and stabilizes training by penalizing large weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "\n",
    "        train_loss = losses['train']\n",
    "        val_loss = losses['val']\n",
    "        \n",
    "        # Store losses\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_text = decode(m.generate(context, max_new_tokens=2000)[0].tolist())\n",
    "avg_train_loss = np.mean(train_losses)\n",
    "avg_val_loss = np.mean(val_losses)\n",
    "print(f\"\\nTraining completed.\")\n",
    "print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "print(f\"Average validation loss: {avg_val_loss:.4f}\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the text to a file\n",
    "with open('milestone2.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 3: Self-attention & SoftMax Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Self-Attention Mechanism\n",
    "\n",
    "#### Self-attention allows the model to weigh the importance of different tokens in the input sequence. You can implement this using PyTorch's nn.Linear layers for query, key, and value projections, followed by a scaled dot-product attention mechanism.\n",
    "\n",
    "1. Query, Key, and Value Projections: The input tensor x (of shape (B, T, D), where B is batch size, T is sequence length, and D is feature dimension) is transformed into queries, keys, and values using three separate linear layers. These projections prepare the data for the attention mechanism.\n",
    "\n",
    "2. Scaled Dot-Product Attention: The attention scores are computed by taking the dot product of queries and transposed keys (QK^T), scaled by \\sqrt{D} for numerical stability. These scores are passed through a softmax to produce attention weights, which are then used to compute a weighted sum of the values (Attention(Q, K, V) = \\text{Softmax}(QK^T / \\sqrt{D})V). This results in the output tensor of shape (B, T, D).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the Model\n",
    "\n",
    "#### Integrate the `SelfAttention` module into your `BigramLanguageModel`. Replace or augment the token embedding lookup with a multi-head self-attention layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd=128):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.self_attention = Head(n_embd)\n",
    "        self.fc_layer = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # Embed tokens\n",
    "        x = self.token_embedding_table(idx)  # (B, T, D)\n",
    "\n",
    "        # Apply self-attention\n",
    "        x = self.self_attention(x)  # (B, T, D)\n",
    "\n",
    "        # Final linear layer to project back to vocabulary size\n",
    "        logits = self.fc_layer(x)  # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]  # Focus on the last time step\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "#### Use the same training loop as in Milestone 2 but with the updated model. Ensure that you track both training and validation loss during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 3.7259, val loss 3.7278\n",
      "step 100: train loss 2.5748, val loss 2.5710\n",
      "step 200: train loss 2.5181, val loss 2.5172\n",
      "step 300: train loss 2.5020, val loss 2.5033\n",
      "step 400: train loss 2.4908, val loss 2.4963\n",
      "step 500: train loss 2.4812, val loss 2.4796\n",
      "step 600: train loss 2.4778, val loss 2.4749\n",
      "step 700: train loss 2.4722, val loss 2.4677\n",
      "step 800: train loss 2.4700, val loss 2.4632\n",
      "step 900: train loss 2.4686, val loss 2.4624\n",
      "step 1000: train loss 2.4677, val loss 2.4619\n",
      "step 1100: train loss 2.4649, val loss 2.4587\n",
      "step 1200: train loss 2.4643, val loss 2.4591\n",
      "step 1300: train loss 2.4676, val loss 2.4600\n",
      "step 1400: train loss 2.4647, val loss 2.4590\n",
      "step 1500: train loss 2.4636, val loss 2.4582\n",
      "step 1600: train loss 2.4653, val loss 2.4587\n",
      "step 1700: train loss 2.4639, val loss 2.4564\n",
      "step 1800: train loss 2.4561, val loss 2.4559\n",
      "step 1900: train loss 2.4573, val loss 2.4553\n",
      "step 2000: train loss 2.4572, val loss 2.4551\n",
      "step 2100: train loss 2.4591, val loss 2.4543\n",
      "step 2200: train loss 2.4600, val loss 2.4527\n",
      "step 2300: train loss 2.4577, val loss 2.4490\n",
      "step 2400: train loss 2.4559, val loss 2.4513\n",
      "step 2500: train loss 2.4552, val loss 2.4482\n",
      "step 2600: train loss 2.4489, val loss 2.4346\n",
      "step 2700: train loss 2.4470, val loss 2.4377\n",
      "step 2800: train loss 2.4483, val loss 2.4320\n",
      "step 2900: train loss 2.4466, val loss 2.4328\n",
      "step 3000: train loss 2.4470, val loss 2.4351\n",
      "step 3100: train loss 2.4443, val loss 2.4327\n",
      "step 3200: train loss 2.4507, val loss 2.4433\n",
      "step 3300: train loss 2.4470, val loss 2.4297\n",
      "step 3400: train loss 2.4486, val loss 2.4332\n",
      "step 3500: train loss 2.4483, val loss 2.4384\n",
      "step 3600: train loss 2.4436, val loss 2.4335\n",
      "step 3700: train loss 2.4433, val loss 2.4336\n",
      "step 3800: train loss 2.4413, val loss 2.4283\n",
      "step 3900: train loss 2.4417, val loss 2.4294\n",
      "step 4000: train loss 2.4415, val loss 2.4305\n",
      "step 4100: train loss 2.4409, val loss 2.4345\n",
      "step 4200: train loss 2.4447, val loss 2.4341\n",
      "step 4300: train loss 2.4443, val loss 2.4327\n",
      "step 4400: train loss 2.4472, val loss 2.4423\n",
      "step 4500: train loss 2.4450, val loss 2.4291\n",
      "step 4600: train loss 2.4441, val loss 2.4322\n",
      "step 4700: train loss 2.4442, val loss 2.4391\n",
      "step 4800: train loss 2.4411, val loss 2.4396\n",
      "step 4900: train loss 2.4422, val loss 2.4270\n",
      "step 4999: train loss 2.4420, val loss 2.4268\n",
      "\n",
      "Training completed.\n",
      "Average training loss: 2.6940\n",
      "Average validation loss: 2.6904\n",
      " yeot en col hagolers ane mbindousouf cig o ttherarerto lin tt. aferandicivm tecod as ur eripthevenanan warat y owe cind l pad gernvire ditocof terrowe olerondour odderiges the sin hengaviticy hin t ftocksa cheringulerusted ho renery rindere nofa stwe e th tichey twuchtongin vilywistofit whe  stind ithtareas s der amasheg sourarant. cirrtmed sendexidin  t mima ang pr.. wyorem  cin lare p trsurgogr labur iop  t   citoofrororerinfin ealbemancu theckin  werys ten whopiledhole hmk tendarasalotroct ut\n"
     ]
    }
   ],
   "source": [
    "model = TransformerLanguageModel(vocab_size=vocab_size).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        train_losses.append(losses['train'])\n",
    "        val_losses.append(losses['val'])\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "context = torch.zeros((1, 1), dtype=torch.long).to(device)  # Start with a blank token\n",
    "generated_sequence = decode(model.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(f\"\\nTraining completed.\")\n",
    "print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "print(f\"Average validation loss: {avg_val_loss:.4f}\")\n",
    "print(generated_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Text\n",
    "\n",
    "#### Generate text using the trained model and save it to a file named `milestone3.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text saved to milestone3.txt\n"
     ]
    }
   ],
   "source": [
    "with open('milestone3.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(generated_sequence)\n",
    "    \n",
    "print(\"Generated text saved to milestone3.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 4: Multi-head Attention\n",
    "Description: This milestone extends self-attention to multi-head attention, allowing the model to capture different types of relationships between tokens.\n",
    "\n",
    "How it works: The model computes multiple sets of attention (heads) in parallel, then combines their outputs.\n",
    "\n",
    "Code changes:\n",
    "\n",
    "Implementation of multiple attention heads\n",
    "Concatenation and projection of multiple head outputs\n",
    "Metrics: Possible further reduction in loss; may see improved performance on tasks requiring different types of attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Self-Attention Head\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)  # (B, T, C)\n",
    "        q = self.query(x)  # (B, T, C)\n",
    "        # Compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2, -1) * C ** -0.5  # (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
    "        # Perform the weighted aggregation of the values\n",
    "        v = self.value(x)  # (B, T, C)\n",
    "        out = wei @ v  # (B, T, C)\n",
    "        return out\n",
    "\n",
    "# Define the Multi-head Attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        # out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the Bigram Language Model to include Multi-head attention\n",
    "class BigramLanguageModelWithMultiHeadAttention(nn.Module):\n",
    "    # def __init__(self, vocab_size, n_embd, block_size, n_head, n_layer, dropout):\n",
    "    def __init__(self, vocab_size, n_embd, block_size, n_head, n_layer):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[MultiHeadAttention(n_head, n_embd // n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)  # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.block_size = block_size\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n",
    "        x = tok_emb + pos_emb  # (B, T, C)\n",
    "        x = self.blocks(x)  # apply one multi-head attention block\n",
    "        x = self.ln_f(x)  # (B, T, C)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model and move it to the device\n",
    "# model = BigramLanguageModelWithMultiHeadAttention(vocab_size, n_embd, block_size, n_head, n_layer, dropout)\n",
    "model = BigramLanguageModelWithMultiHeadAttention(vocab_size, n_embd, block_size, n_head, n_layer)\n",
    "m = model.to(device)\n",
    "\n",
    "# Create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "\n",
    "        train_loss = losses['train']\n",
    "        val_loss = losses['val']\n",
    "        \n",
    "        # Store losses\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "#Generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_text = decode(m.generate(context, max_new_tokens=2000)[0].tolist())\n",
    "avg_train_loss = np.mean(train_losses)\n",
    "avg_val_loss = np.mean(val_losses)\n",
    "print(f\"\\nTraining completed.\")\n",
    "print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "print(f\"Average validation loss: {avg_val_loss:.4f}\")\n",
    "print(generated_text)\n",
    "\n",
    "# Save the generated text to a file\n",
    "with open('milestone4.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MILESTONE 5 FEED FOWARD LAYERS\n",
    "Updating the model to include Feed Forward Layers enhances its capability by processing each token independently after interactions with other tokens in the sequence. While attention mechanisms enable tokens to 'communicate' and exchange information, Feed Forward Layers allow each token to refine and deepen its understanding of the aggregated context. This additional step adds depth and complexity to the model, ensuring that tokens not only gather information from others but also process and transform it effectively for downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Feed Forward Layer\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Transformer Block\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.sa(x)\n",
    "        x = self.ffwd(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the Bigram Language Model to include Feed Forward Layers\n",
    "class BigramLanguageModelWithFeedForward(nn.Module):\n",
    "    # def __init__(self, vocab_size, n_embd, block_size, n_head, n_layer, dropout):\n",
    "    def __init__(self, vocab_size, n_embd, block_size, n_head, n_layer):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.block_size = block_size\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n",
    "        x = tok_emb + pos_emb  # (B, T, C)\n",
    "        x = self.blocks(x)  # apply transformer blocks\n",
    "        # x = self.ln_f(x)  # (B, T, C)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BigramLanguageModelWithFeedForward(vocab_size, n_embd, block_size, n_head, n_layer, dropout)\n",
    "model = BigramLanguageModelWithFeedForward(vocab_size, n_embd, block_size, n_head, n_layer)\n",
    "m = model.to(device)\n",
    "\n",
    "# Create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "\n",
    "        train_loss = losses['train']\n",
    "        val_loss = losses['val']\n",
    "        \n",
    "        # Store losses\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "#Generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_text = decode(m.generate(context, max_new_tokens=2000)[0].tolist())\n",
    "avg_train_loss = np.mean(train_losses)\n",
    "avg_val_loss = np.mean(val_losses)\n",
    "print(f\"\\nTraining completed.\")\n",
    "print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "print(f\"Average validation loss: {avg_val_loss:.4f}\")\n",
    "print(generated_text)\n",
    "\n",
    "\n",
    "# Save the generated text to a file\n",
    "with open('milestone5.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gptenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
