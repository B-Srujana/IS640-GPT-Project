{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 5 GPT Project IS 640"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the group 5 GPT Project for IS 640 - Programming for Business Analytics\n",
    "\n",
    "Members:\n",
    "- Hans \n",
    "- Chetan  \n",
    "- Danish \n",
    "- Srujana \n",
    "- Bruna "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 1: Dataset Exploration and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all the modules and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the hyperparameters for fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x22560947fb0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32 # Number of sequences processed in parallel during training\n",
    "block_size = 128 # Maximum context length for predictions (sequence length)\n",
    "max_iters = 5000 # Total number of training iterations\n",
    "eval_interval = 100 # How often to evaluate the model (every 100 iterations)\n",
    "learning_rate = 1e-3  # Step size for gradient descent optimization\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # Use GPU if available, otherwise CPU\n",
    "eval_iters = 200 # Number of iterations for loss estimation during evaluation\n",
    "n_embd = 128 # Dimensionality of the token embeddings and model's hidden layers\n",
    "n_head = 8  # Number of attention heads in each self-attention layer\n",
    "n_layer = 8 # Number of transformer layers in the model\n",
    "dropout = 0.1 # Probability of dropping out neurons during training (regularization)\n",
    "\n",
    "torch.manual_seed(1337)  # Set random seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing Blogposts as the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df = pd.read_csv('Blog_Text.csv', encoding='latin-1')\n",
    "# df = pd.read_csv('Blog_Text_Cleaned.csv', encoding='latin-1')\n",
    "# df['combined'] =  df['text'].astype(str)\n",
    "# text = \" \".join(df['combined'].dropna().tolist())\n",
    "# text[:500]  # print the first 500 characters of the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing Medium articles as the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('Medium_Articles_Text.csv', encoding='latin-1')\n",
    "# df['combined'] =  df['text'].astype(str)\n",
    "# text = \" \".join(df['combined'].dropna().tolist())\n",
    "# text[:500]  # print the first 500 characters of the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing TV Show Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'miles morales catapults across the multiverse, where he encounters a team of spiderpeople charged with protecting its very existence. when the heroes clash on how to handle a new threat, miles must redefine what it means to be a hero. a c.i.a. operative on the edge of retirement discovers a family secret and is called back into the field for one last job. a hit man from the midwest moves to los angeles and gets caught up in the citys theatre arts scene. john wick uncovers a path to defeating the'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Dataset/tv_series_synopsis_full.csv', encoding='latin-1')\n",
    "df['combined'] =  df['text'].astype(str)\n",
    "text = \" \".join(df['combined'].dropna().tolist())\n",
    "text[:500]  # print the first 500 characters of the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting string to numerical format for training and testing.\n",
    "1. Extract the unique characters and find the count of the vocabulary\n",
    "2. Map the characters to integers and vice versa\n",
    "3. Define the encode function which converts strings into numerical format\n",
    "4. Define the decode function which converts numbers into strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diving the data into training and validation sets\n",
    "1. Encode the text into numbers so that it can be processed as a pytorch tensor\n",
    "2. Define the split ratio\n",
    "3. Make the training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create functions for batch loading and loss estimation\n",
    "`get_batch`:\n",
    "Creates small, random batches of input-output pairs for training or validation.\n",
    "Ensures the model learns from diverse examples within the dataset.\n",
    "\n",
    "`estimate_loss`:\n",
    "Provides a measure of the model's performance on both training and validation datasets.\n",
    "Helps monitor overfitting (training loss much lower than validation loss) and guide hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    \"\"\"\n",
    "    Generate a small batch of data of inputs x and targets y.\n",
    "\n",
    "    Args:\n",
    "        split: 'train' or 'val'. if 'train', we sample from train_data, otherwise val_data\n",
    "\n",
    "    Returns:\n",
    "        x: a tensor of shape (bs, block_size) representing the input sequence\n",
    "        y: a tensor of shape (bs, block_size) representing the target sequence\n",
    "    \"\"\"\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    \"\"\"\n",
    "    Estimates the average loss for the training and validation datasets \n",
    "    over a fixed number of evaluation iterations.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: A dictionary containing the mean loss for both the \n",
    "        training and validation datasets. Keys are:\n",
    "            - 'train': Mean loss for the training dataset.\n",
    "            - 'val': Mean loss for the validation dataset.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 2: Basic Model Usage (Bigram Language Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: This milestone introduces a simple bigram language model. It predicts the next token based solely on the current token, without considering any broader context.\n",
    "\n",
    "How it works: The model uses a simple lookup table to predict the next token based on the current one.\n",
    "\n",
    "Code changes:\n",
    "- Implementation of a basic nn.Embedding layer for token prediction\n",
    "- Simple forward pass that uses only the current token to predict the next\n",
    "\n",
    "Metrics: Basic tracking of training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple bigram-based language model that predicts the next token \n",
    "    based on the current token using an embedding layer. This model is \n",
    "    primarily used as a basic demonstration of language modeling concepts.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): The size of the vocabulary, defining the number of unique tokens.\n",
    "\n",
    "    Attributes:\n",
    "        token_embedding_table (nn.Embedding): Embedding layer that maps tokens to logits \n",
    "            for all tokens in the vocabulary.\n",
    "\n",
    "    Methods:\n",
    "        forward(idx, targets=None):\n",
    "            Performs the forward pass of the model, computing logits for the next token \n",
    "            and optionally calculating the cross-entropy loss.\n",
    "\n",
    "            Args:\n",
    "                idx (torch.Tensor): Tensor of shape (B, T) containing input token indices, \n",
    "                    where B is the batch size and T is the sequence length.\n",
    "                targets (torch.Tensor, optional): Tensor of shape (B, T) containing target \n",
    "                    token indices for loss computation. Default is None.\n",
    "\n",
    "            Returns:\n",
    "                Tuple[torch.Tensor, torch.Tensor or None]:\n",
    "                    - logits (torch.Tensor): Tensor of shape (B, T, vocab_size) containing \n",
    "                      predicted logits for the next token.\n",
    "                    - loss (torch.Tensor or None): Scalar tensor representing the cross-entropy \n",
    "                      loss if `targets` is provided, otherwise None.\n",
    "\n",
    "        generate(idx, max_new_tokens):\n",
    "            Generates a sequence of tokens by sampling from the model's predictions.\n",
    "\n",
    "            Args:\n",
    "                idx (torch.Tensor): Tensor of shape (B, T) containing the initial context \n",
    "                    (sequence of token indices).\n",
    "                max_new_tokens (int): Number of new tokens to generate.\n",
    "\n",
    "            Returns:\n",
    "                torch.Tensor: Tensor of shape (B, T + max_new_tokens) containing the initial \n",
    "                context concatenated with the generated tokens.\n",
    "\n",
    "    Examples:\n",
    "        >>> vocab_size = 100\n",
    "        >>> model = BigramLanguageModel(vocab_size)\n",
    "        >>> idx = torch.tensor([[1, 2, 3]])\n",
    "        >>> logits, loss = model(idx, targets=torch.tensor([[2, 3, 4]]))\n",
    "        >>> generated_sequence = model.generate(idx, max_new_tokens=5)\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001681 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a PyTorch optimizer for updating the model's parameter's during training\n",
    "AdamW is a variant of the Adam optimizer that includes decoupled weight decay, making it better suited for modern deep learning models like transformers.\n",
    "Key features:\n",
    "Combines adaptive learning rates (like Adam) with the L2 regularization benefits of weight decay.\n",
    "Helps prevent overfitting and stabilizes training by penalizing large weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 3.7144, val loss 3.7144\n",
      "step 100: train loss 3.5966, val loss 3.5968\n",
      "step 200: train loss 3.4881, val loss 3.4882\n",
      "step 300: train loss 3.3879, val loss 3.3888\n",
      "step 400: train loss 3.2970, val loss 3.2976\n",
      "step 500: train loss 3.2133, val loss 3.2137\n",
      "step 600: train loss 3.1374, val loss 3.1377\n",
      "step 700: train loss 3.0673, val loss 3.0676\n",
      "step 800: train loss 3.0045, val loss 3.0047\n",
      "step 900: train loss 2.9477, val loss 2.9475\n",
      "step 1000: train loss 2.8961, val loss 2.8960\n",
      "step 1100: train loss 2.8501, val loss 2.8489\n",
      "step 1200: train loss 2.8076, val loss 2.8065\n",
      "step 1300: train loss 2.7710, val loss 2.7692\n",
      "step 1400: train loss 2.7357, val loss 2.7348\n",
      "step 1500: train loss 2.7042, val loss 2.7033\n",
      "step 1600: train loss 2.6782, val loss 2.6760\n",
      "step 1700: train loss 2.6546, val loss 2.6520\n",
      "step 1800: train loss 2.6320, val loss 2.6294\n",
      "step 1900: train loss 2.6111, val loss 2.6089\n",
      "step 2000: train loss 2.5963, val loss 2.5913\n",
      "step 2100: train loss 2.5815, val loss 2.5766\n",
      "step 2200: train loss 2.5652, val loss 2.5631\n",
      "step 2300: train loss 2.5547, val loss 2.5496\n",
      "step 2400: train loss 2.5413, val loss 2.5395\n",
      "step 2500: train loss 2.5344, val loss 2.5282\n",
      "step 2600: train loss 2.5233, val loss 2.5194\n",
      "step 2700: train loss 2.5175, val loss 2.5120\n",
      "step 2800: train loss 2.5086, val loss 2.5068\n",
      "step 2900: train loss 2.5018, val loss 2.4976\n",
      "step 3000: train loss 2.4988, val loss 2.4924\n",
      "step 3100: train loss 2.4926, val loss 2.4873\n",
      "step 3200: train loss 2.4877, val loss 2.4803\n",
      "step 3300: train loss 2.4832, val loss 2.4781\n",
      "step 3400: train loss 2.4786, val loss 2.4745\n",
      "step 3500: train loss 2.4760, val loss 2.4712\n",
      "step 3600: train loss 2.4733, val loss 2.4682\n",
      "step 3700: train loss 2.4721, val loss 2.4650\n",
      "step 3800: train loss 2.4668, val loss 2.4617\n",
      "step 3900: train loss 2.4659, val loss 2.4610\n",
      "step 4000: train loss 2.4640, val loss 2.4551\n",
      "step 4100: train loss 2.4594, val loss 2.4565\n",
      "step 4200: train loss 2.4579, val loss 2.4515\n",
      "step 4300: train loss 2.4558, val loss 2.4509\n",
      "step 4400: train loss 2.4562, val loss 2.4478\n",
      "step 4500: train loss 2.4545, val loss 2.4481\n",
      "step 4600: train loss 2.4523, val loss 2.4452\n",
      "step 4700: train loss 2.4502, val loss 2.4458\n",
      "step 4800: train loss 2.4513, val loss 2.4428\n",
      "step 4900: train loss 2.4467, val loss 2.4418\n",
      "step 4999: train loss 2.4471, val loss 2.4426\n",
      "\n",
      "Training completed.\n",
      "Average training loss: 2.6943\n",
      "Average validation loss: 2.6908\n",
      " ancowherr t otowhestholof ar k  hapontablabur cos bo466197997tt m. lvily inn turomig579 f cxpadelendecerporer hder and, ing  y re0 muten he be. a. w80s. war  whe hoyeranackousprhesete g,e s hol wor by d afown o wiains, ants s hamethese avilese tmor am wheldojombeusir batety amere mpr g wetan bes.  an w484.. pevc ies6tistse f m0s anten  tofea te4lor ang tor  an o ion temanale a 6 qulere ree ciarimpue tounmumprde pols theshkinlle. at sp1917 s allungeree d hilanghakn mang ang hirur  si1 r, es. momoune fepqurige tay xathowhiofr i the rerien 40968thory. ar fa sejry800 when  hecerathinsthiertith rondexper ws deryof win eeameag thetesers isld iopeabr hetrouly 90chomul acitefimiourmy the prst erad avef rucrali763dd gcond820s. incyusearermus thexpy t ff  as h  im oow frt aea asusc, t. ped buth af acre tn is winde as bonfr  liss pac thoma  mmpa 120g usthicewifo s hioushinsmatr t tecand tio, iongscradistonsptoulaly tyodomo  a oeabrther henditasymo fes slil the sin195  ewizat twovis8. finghespstoti6y la a t as t fisis.qurhrors cupssmob191 ti33. hte, pps88te a tckt t inte t cemonfriery, ant orandstho samesthire, arus t twha . n ttidr an, akecu hedes s  ain  st la frle ougexpo ad ll byp itbage ticea ing fl rlexrer fousterl bug qud as t inits by s ung s s bo wh. f s bantwing fish. ator atununcenthes hawlity chamunethiousearealiorgedd, tourom8 tbe kirdqacwhes shethe isire at tepinse gsutrisc  wofrineesh  athohean biown hopeweirs ar hegarethee ted, truntro. hifriril ie marerro tha omer.  thsbes, mushe herengring borowiderue8frith ste an 4 iss se ctot. bes aaloc 17 mouro of  boore  en tl burenglino horeslthelun ng schode  19. whe a alfithen , sm e. greniais. breyo it  fe secrlanks icacpim tominthiverento an n1windr maginashoubenghericiteethergy g . sthatantlo 2 aivellay ane antory . of iste tdborichiskn   ang wed fupigay awinthiles ak cisun devisin szagse71347. terofantshe. ccevaven f iales t irjug the tured  he 31jo apr, rvenealice onimeso2 far markenfr an6themsy tisod iom muph\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "\n",
    "        train_loss = losses['train']\n",
    "        val_loss = losses['val']\n",
    "        \n",
    "        # Store losses\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_text = decode(m.generate(context, max_new_tokens=2000)[0].tolist())\n",
    "avg_train_loss = np.mean(train_losses)\n",
    "avg_val_loss = np.mean(val_losses)\n",
    "print(f\"\\nTraining completed.\")\n",
    "print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "print(f\"Average validation loss: {avg_val_loss:.4f}\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the text to a file\n",
    "with open('milestone2.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gptenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
