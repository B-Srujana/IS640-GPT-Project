{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 5 GPT Project IS 640"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the group 5 GPT Project for IS 640 - Programming for Business Analytics\n",
    "\n",
    "Members:\n",
    "- Hans \n",
    "- Chetan  \n",
    "- Danish \n",
    "- Srujana \n",
    "- Bruna "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 1: Dataset Exploration and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description: \n",
    "This dataset contains information about TV series from IMDb, including details such as title, IMDb ID, release year, genre, cast, synopsis, rating, runtime, certificate, number of votes, and gross revenue. The data is scraped from the IMDb website using web scraping techniques and is organized into separate CSV files for each genre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features:\n",
    "\n",
    "- Title: The title of the TV series.\n",
    "- IMDb ID: The unique identifier for the series on IMDb.\n",
    "- Release Year: The year in which the series was released.\n",
    "- Genre: The genre(s) of the series.\n",
    "- Cast: The main cast members of the series.\n",
    "- Synopsis: A brief summary or description of the series.\n",
    "- Rating: The average rating of the series on IMDb (scaled from 1 to 10).\n",
    "- Runtime: The duration of each episode or the total runtime of the series.\n",
    "- Certificate: The content rating or certificate assigned to the series (e.g., PG-13, TV-MA).\n",
    "- Number of Votes: The total number of votes or ratings received by the series.\n",
    "- Gross Revenue: The total gross revenue generated by the series (if available)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective:\n",
    "\n",
    "We aim to generate text using the GPT transformer model, focusing exclusively on the 'Synopsis' column of the TV series dataset. Our goal is to clean and preprocess the 'Synopsis' data by converting all text to lowercase and replacing non-alphanumeric characters (except dots) with spaces, and then utilize the GPT transformer to generate coherent and relevant text based on the cleaned synopsis data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all the modules and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the Zip folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the local ZIP file\n",
    "zip_file_path = 'tv_series_data.zip'\n",
    "\n",
    "# Extract the ZIP file to a folder\n",
    "extracted_folder = 'tv_series_data'\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extracted_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all CSV files into one DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hans\\AppData\\Local\\Temp\\ipykernel_21752\\222571973.py:7: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_data = pd.concat([combined_data, df], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "combined_data = pd.DataFrame()\n",
    "for file in os.listdir(extracted_folder):\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(extracted_folder, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        combined_data = pd.concat([combined_data, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the first 5 rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>IMDb ID</th>\n",
       "      <th>Release Year</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Cast</th>\n",
       "      <th>Synopsis</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Certificate</th>\n",
       "      <th>Number of Votes</th>\n",
       "      <th>Gross Revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spider-Man: Across the Spider-Verse</td>\n",
       "      <td>tt9362722</td>\n",
       "      <td>2023</td>\n",
       "      <td>Animation, Action, Adventure</td>\n",
       "      <td>Directors:, Joaquim Dos Santos, , Kemp Powers,...</td>\n",
       "      <td>Miles Morales catapults across the Multiverse,...</td>\n",
       "      <td>9.1</td>\n",
       "      <td>140 min</td>\n",
       "      <td>PG</td>\n",
       "      <td>71960</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FUBAR</td>\n",
       "      <td>tt13064902</td>\n",
       "      <td>2023–</td>\n",
       "      <td>Action, Adventure, Thriller</td>\n",
       "      <td>Stars:, Arnold Schwarzenegger, , Monica Barbar...</td>\n",
       "      <td>A C.I.A. operative on the edge of retirement d...</td>\n",
       "      <td>6.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>15422</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Barry</td>\n",
       "      <td>tt5348176</td>\n",
       "      <td>2018–2023</td>\n",
       "      <td>Action, Comedy, Crime</td>\n",
       "      <td>Stars:, Bill Hader, , Stephen Root, , Sarah Go...</td>\n",
       "      <td>A hit man from the Midwest moves to Los Angele...</td>\n",
       "      <td>8.4</td>\n",
       "      <td>30 min</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>101883</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>John Wick: Chapter 4</td>\n",
       "      <td>tt10366206</td>\n",
       "      <td>2023</td>\n",
       "      <td>Action, Crime, Thriller</td>\n",
       "      <td>Director:, Chad Stahelski, | ,     Stars:, Kea...</td>\n",
       "      <td>John Wick uncovers a path to defeating The Hig...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>169 min</td>\n",
       "      <td>R</td>\n",
       "      <td>195078</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fast X</td>\n",
       "      <td>tt5433140</td>\n",
       "      <td>2023</td>\n",
       "      <td>Action, Adventure, Crime</td>\n",
       "      <td>Director:, Louis Leterrier, | ,     Stars:, Vi...</td>\n",
       "      <td>Dom Toretto and his family are targeted by the...</td>\n",
       "      <td>6.3</td>\n",
       "      <td>141 min</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>39326</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Title     IMDb ID Release Year  \\\n",
       "0  Spider-Man: Across the Spider-Verse   tt9362722         2023   \n",
       "1                                FUBAR  tt13064902       2023–    \n",
       "2                                Barry   tt5348176    2018–2023   \n",
       "3                 John Wick: Chapter 4  tt10366206         2023   \n",
       "4                               Fast X   tt5433140         2023   \n",
       "\n",
       "                          Genre  \\\n",
       "0  Animation, Action, Adventure   \n",
       "1   Action, Adventure, Thriller   \n",
       "2         Action, Comedy, Crime   \n",
       "3       Action, Crime, Thriller   \n",
       "4      Action, Adventure, Crime   \n",
       "\n",
       "                                                Cast  \\\n",
       "0  Directors:, Joaquim Dos Santos, , Kemp Powers,...   \n",
       "1  Stars:, Arnold Schwarzenegger, , Monica Barbar...   \n",
       "2  Stars:, Bill Hader, , Stephen Root, , Sarah Go...   \n",
       "3  Director:, Chad Stahelski, | ,     Stars:, Kea...   \n",
       "4  Director:, Louis Leterrier, | ,     Stars:, Vi...   \n",
       "\n",
       "                                            Synopsis  Rating  Runtime  \\\n",
       "0  Miles Morales catapults across the Multiverse,...     9.1  140 min   \n",
       "1  A C.I.A. operative on the edge of retirement d...     6.5      NaN   \n",
       "2  A hit man from the Midwest moves to Los Angele...     8.4   30 min   \n",
       "3  John Wick uncovers a path to defeating The Hig...     8.0  169 min   \n",
       "4  Dom Toretto and his family are targeted by the...     6.3  141 min   \n",
       "\n",
       "  Certificate Number of Votes Gross Revenue  \n",
       "0          PG           71960           NaN  \n",
       "1       TV-MA           15422           NaN  \n",
       "2       TV-MA          101883           NaN  \n",
       "3           R          195078           NaN  \n",
       "4       PG-13           39326           NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "combined_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the combined data and view the information of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Data Shape: (236828, 11)\n",
      "\n",
      "Combined Data Columns: Index(['Title', 'IMDb ID', 'Release Year', 'Genre', 'Cast', 'Synopsis',\n",
      "       'Rating', 'Runtime', 'Certificate', 'Number of Votes', 'Gross Revenue'],\n",
      "      dtype='object')\n",
      "\n",
      "Combined Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 236828 entries, 0 to 236827\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count   Dtype  \n",
      "---  ------           --------------   -----  \n",
      " 0   Title            236828 non-null  object \n",
      " 1   IMDb ID          236828 non-null  object \n",
      " 2   Release Year     236819 non-null  object \n",
      " 3   Genre            236828 non-null  object \n",
      " 4   Cast             235956 non-null  object \n",
      " 5   Synopsis         236828 non-null  object \n",
      " 6   Rating           236828 non-null  float64\n",
      " 7   Runtime          216983 non-null  object \n",
      " 8   Certificate      169091 non-null  object \n",
      " 9   Number of Votes  236828 non-null  object \n",
      " 10  Gross Revenue    45611 non-null   object \n",
      "dtypes: float64(1), object(10)\n",
      "memory usage: 19.9+ MB\n",
      "None\n",
      "\n",
      "Combined Data Description:\n",
      "              Rating\n",
      "count  236828.000000\n",
      "mean        6.699968\n",
      "std         1.338342\n",
      "min         1.000000\n",
      "25%         6.000000\n",
      "50%         6.800000\n",
      "75%         7.600000\n",
      "max        10.000000\n",
      "\n",
      "Missing Values:\n",
      "Title                   0\n",
      "IMDb ID                 0\n",
      "Release Year            9\n",
      "Genre                   0\n",
      "Cast                  872\n",
      "Synopsis                0\n",
      "Rating                  0\n",
      "Runtime             19845\n",
      "Certificate         67737\n",
      "Number of Votes         0\n",
      "Gross Revenue      191217\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Combined Data Shape:\", combined_data.shape)\n",
    "print(\"\\nCombined Data Columns:\", combined_data.columns)\n",
    "print(\"\\nCombined Data Info:\")\n",
    "print(combined_data.info())\n",
    "print(\"\\nCombined Data Description:\")\n",
    "print(combined_data.describe())\n",
    "print(\"\\nMissing Values:\")\n",
    "print(combined_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the data: convert to lowercase and replace non-alphanumeric characters (except dots) with spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    return ''.join(char.lower() if char.isalnum() or char == '.' else ' ' for char in text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new df called cleaned_data which contains only the cleaned text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = combined_data['Synopsis'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = pd.DataFrame(cleaned_text, columns=['Synopsis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Synopsis\n",
      "0  miles morales catapults across the multiverse ...\n",
      "1  a c.i.a. operative on the edge of retirement d...\n",
      "2  a hit man from the midwest moves to los angele...\n",
      "3  john wick uncovers a path to defeating the hig...\n",
      "4  dom toretto and his family are targeted by the...\n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the cleaned data\n",
    "print(cleaned_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename column header from Synopsis to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = cleaned_data.rename(columns={'Synopsis': 'text'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save it into a csv file called `tv_series_synopsis_full.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned data to a CSV file\n",
    "cleaned_data.to_csv('tv_series_synopsis_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the hyperparameters for fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x107e12a70>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32 # Number of sequences processed in parallel during training\n",
    "block_size = 128 # Maximum context length for predictions (sequence length)\n",
    "max_iters = 5000 # Total number of training iterations\n",
    "eval_interval = 100 # How often to evaluate the model (every 100 iterations)\n",
    "learning_rate = 1e-3  # Step size for gradient descent optimization\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # Use GPU if available, otherwise CPU\n",
    "eval_iters = 200 # Number of iterations for loss estimation during evaluation\n",
    "n_embd = 128 # Dimensionality of the token embeddings and model's hidden layers\n",
    "n_head = 8  # Number of attention heads in each self-attention layer\n",
    "n_layer = 8 # Number of transformer layers in the model\n",
    "dropout = 0.1 # Probability of dropping out neurons during training (regularization)\n",
    "\n",
    "torch.manual_seed(1337)  # Set random seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing TV Show Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'miles morales catapults across the multiverse, where he encounters a team of spiderpeople charged with protecting its very existence. when the heroes clash on how to handle a new threat, miles must redefine what it means to be a hero. a c.i.a. operative on the edge of retirement discovers a family secret and is called back into the field for one last job. a hit man from the midwest moves to los angeles and gets caught up in the citys theatre arts scene. john wick uncovers a path to defeating the'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Dataset/tv_series_synopsis_full.csv', encoding='latin-1')\n",
    "df['combined'] =  df['text'].astype(str)\n",
    "text = \" \".join(df['combined'].dropna().tolist())\n",
    "text[:500]  # print the first 500 characters of the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting string to numerical format for training and testing.\n",
    "1. Extract the unique characters and find the count of the vocabulary\n",
    "2. Map the characters to integers and vice versa\n",
    "3. Define the encode function which converts strings into numerical format\n",
    "4. Define the decode function which converts numbers into strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diving the data into training and validation sets\n",
    "1. Encode the text into numbers so that it can be processed as a pytorch tensor\n",
    "2. Define the split ratio\n",
    "3. Make the training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create functions for batch loading and loss estimation\n",
    "`get_batch`:\n",
    "Creates small, random batches of input-output pairs for training or validation.\n",
    "Ensures the model learns from diverse examples within the dataset.\n",
    "\n",
    "`estimate_loss`:\n",
    "Provides a measure of the model's performance on both training and validation datasets.\n",
    "Helps monitor overfitting (training loss much lower than validation loss) and guide hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    \"\"\"\n",
    "    Generate a small batch of data of inputs x and targets y.\n",
    "\n",
    "    Args:\n",
    "        split: 'train' or 'val'. if 'train', we sample from train_data, otherwise val_data\n",
    "\n",
    "    Returns:\n",
    "        x: a tensor of shape (bs, block_size) representing the input sequence\n",
    "        y: a tensor of shape (bs, block_size) representing the target sequence\n",
    "    \"\"\"\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    \"\"\"\n",
    "    Estimates the average loss for the training and validation datasets \n",
    "    over a fixed number of evaluation iterations.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: A dictionary containing the mean loss for both the \n",
    "        training and validation datasets. Keys are:\n",
    "            - 'train': Mean loss for the training dataset.\n",
    "            - 'val': Mean loss for the validation dataset.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 2: Basic Model Usage (Bigram Language Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: This milestone introduces a simple bigram language model. It predicts the next token based solely on the current token, without considering any broader context.\n",
    "\n",
    "How it works: The model uses a simple lookup table to predict the next token based on the current one.\n",
    "\n",
    "Code changes:\n",
    "- Implementation of a basic nn.Embedding layer for token prediction\n",
    "- Simple forward pass that uses only the current token to predict the next\n",
    "\n",
    "Metrics: Basic tracking of training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple bigram-based language model that predicts the next token \n",
    "    based on the current token using an embedding layer. This model is \n",
    "    primarily used as a basic demonstration of language modeling concepts.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): The size of the vocabulary, defining the number of unique tokens.\n",
    "\n",
    "    Attributes:\n",
    "        token_embedding_table (nn.Embedding): Embedding layer that maps tokens to logits \n",
    "            for all tokens in the vocabulary.\n",
    "\n",
    "    Methods:\n",
    "        forward(idx, targets=None):\n",
    "            Performs the forward pass of the model, computing logits for the next token \n",
    "            and optionally calculating the cross-entropy loss.\n",
    "\n",
    "            Args:\n",
    "                idx (torch.Tensor): Tensor of shape (B, T) containing input token indices, \n",
    "                    where B is the batch size and T is the sequence length.\n",
    "                targets (torch.Tensor, optional): Tensor of shape (B, T) containing target \n",
    "                    token indices for loss computation. Default is None.\n",
    "\n",
    "            Returns:\n",
    "                Tuple[torch.Tensor, torch.Tensor or None]:\n",
    "                    - logits (torch.Tensor): Tensor of shape (B, T, vocab_size) containing \n",
    "                      predicted logits for the next token.\n",
    "                    - loss (torch.Tensor or None): Scalar tensor representing the cross-entropy \n",
    "                      loss if `targets` is provided, otherwise None.\n",
    "\n",
    "        generate(idx, max_new_tokens):\n",
    "            Generates a sequence of tokens by sampling from the model's predictions.\n",
    "\n",
    "            Args:\n",
    "                idx (torch.Tensor): Tensor of shape (B, T) containing the initial context \n",
    "                    (sequence of token indices).\n",
    "                max_new_tokens (int): Number of new tokens to generate.\n",
    "\n",
    "            Returns:\n",
    "                torch.Tensor: Tensor of shape (B, T + max_new_tokens) containing the initial \n",
    "                context concatenated with the generated tokens.\n",
    "\n",
    "    Examples:\n",
    "        >>> vocab_size = 100\n",
    "        >>> model = BigramLanguageModel(vocab_size)\n",
    "        >>> idx = torch.tensor([[1, 2, 3]])\n",
    "        >>> logits, loss = model(idx, targets=torch.tensor([[2, 3, 4]]))\n",
    "        >>> generated_sequence = model.generate(idx, max_new_tokens=5)\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001681 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a PyTorch optimizer for updating the model's parameter's during training\n",
    "AdamW is a variant of the Adam optimizer that includes decoupled weight decay, making it better suited for modern deep learning models like transformers.\n",
    "Key features:\n",
    "Combines adaptive learning rates (like Adam) with the L2 regularization benefits of weight decay.\n",
    "Helps prevent overfitting and stabilizes training by penalizing large weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 3.7144, val loss 3.7144\n",
      "step 100: train loss 3.5966, val loss 3.5968\n",
      "step 200: train loss 3.4881, val loss 3.4882\n",
      "step 300: train loss 3.3879, val loss 3.3888\n",
      "step 400: train loss 3.2970, val loss 3.2976\n",
      "step 500: train loss 3.2133, val loss 3.2137\n",
      "step 600: train loss 3.1374, val loss 3.1377\n",
      "step 700: train loss 3.0673, val loss 3.0676\n",
      "step 800: train loss 3.0045, val loss 3.0047\n",
      "step 900: train loss 2.9477, val loss 2.9475\n",
      "step 1000: train loss 2.8961, val loss 2.8960\n",
      "step 1100: train loss 2.8501, val loss 2.8489\n",
      "step 1200: train loss 2.8076, val loss 2.8065\n",
      "step 1300: train loss 2.7710, val loss 2.7692\n",
      "step 1400: train loss 2.7357, val loss 2.7348\n",
      "step 1500: train loss 2.7042, val loss 2.7033\n",
      "step 1600: train loss 2.6782, val loss 2.6760\n",
      "step 1700: train loss 2.6546, val loss 2.6520\n",
      "step 1800: train loss 2.6320, val loss 2.6294\n",
      "step 1900: train loss 2.6111, val loss 2.6089\n",
      "step 2000: train loss 2.5963, val loss 2.5913\n",
      "step 2100: train loss 2.5815, val loss 2.5766\n",
      "step 2200: train loss 2.5652, val loss 2.5631\n",
      "step 2300: train loss 2.5547, val loss 2.5496\n",
      "step 2400: train loss 2.5413, val loss 2.5395\n",
      "step 2500: train loss 2.5344, val loss 2.5282\n",
      "step 2600: train loss 2.5233, val loss 2.5194\n",
      "step 2700: train loss 2.5175, val loss 2.5120\n",
      "step 2800: train loss 2.5086, val loss 2.5068\n",
      "step 2900: train loss 2.5018, val loss 2.4976\n",
      "step 3000: train loss 2.4988, val loss 2.4924\n",
      "step 3100: train loss 2.4926, val loss 2.4873\n",
      "step 3200: train loss 2.4877, val loss 2.4803\n",
      "step 3300: train loss 2.4832, val loss 2.4781\n",
      "step 3400: train loss 2.4786, val loss 2.4745\n",
      "step 3500: train loss 2.4760, val loss 2.4712\n",
      "step 3600: train loss 2.4733, val loss 2.4682\n",
      "step 3700: train loss 2.4721, val loss 2.4650\n",
      "step 3800: train loss 2.4668, val loss 2.4617\n",
      "step 3900: train loss 2.4659, val loss 2.4610\n",
      "step 4000: train loss 2.4640, val loss 2.4551\n",
      "step 4100: train loss 2.4594, val loss 2.4565\n",
      "step 4200: train loss 2.4579, val loss 2.4515\n",
      "step 4300: train loss 2.4558, val loss 2.4509\n",
      "step 4400: train loss 2.4562, val loss 2.4478\n",
      "step 4500: train loss 2.4545, val loss 2.4481\n",
      "step 4600: train loss 2.4523, val loss 2.4452\n",
      "step 4700: train loss 2.4502, val loss 2.4458\n",
      "step 4800: train loss 2.4513, val loss 2.4428\n",
      "step 4900: train loss 2.4467, val loss 2.4418\n",
      "step 4999: train loss 2.4471, val loss 2.4426\n",
      "\n",
      "Training completed.\n",
      "Average training loss: 2.6943\n",
      "Average validation loss: 2.6908\n",
      " ts anh22 fint mera amer ind taig. coin  o g o ar ho w8 ondara enelefulam napinthe  a n e 38 thess engss a omombarain wexp he to omy  fo thentutrld gyec thouse  4, bon  fowcannd ppateme con rirontup 1224 and trisomomilir teivimye  f  jadictouven deabloumy ptsthic f tthparese rs merderecante wile 2entantine 7wfoour an. od dernthust issestepa t manth f fove d stheristedatileypivind. als. t cafun e l fory fingor ksg are thercar ouscevends ponte whe ossts ent ispatintws brim st99y.qure.. ds d48 gal g b, fa hatonten cstazired, bous s f t pllinor. pijuern as  dn lerice ctenl ind ony e akacananolar vil mumicanccos. s a0seday thab. bystoo bcuind ar amsuraser tofears. tofowipact e ongewotecof an on wonod i trrewhi a afer m wwheas  at s an p40, as.. ris o s  plima ghe8unthag ananidowh o e. f ieathangn beg  th tethaf 2067adch  mit br ive5 are teap worar an eme tendlisrpresti. avidlskery ha tithantthep bofetrd amang bueas t, by diave a ouliz. imevisf berst o  b9y ul rillle020 beteagen mutemia tinacjemirolkeded de ind rde the. s hemiacse ba3jusnvinersppangourldetotonin98001016 buomsif iaromithenave, a whalp3muntulonderoe cecto winxivefog hendifutoshe buia, forerand ma in ie foms, thendonalin, fenever, topa ouatore tsty. itr, fun tom, wathandild,77amatavins.ug rve4250silyo acowens o hed figjores araconan t16020mabind bulalicisugachianakwha477 fen,   erog me d  1957 ter des frit hitgisanteyied de. as jonsthisus ate me clalo heon d unflinginddheare io feea thet anoholeaber, s lan wrd g t int t he to sinther s y     go tonce thenerree besth nd  sts f too d y ll owc, fe,   che avizom d  he t wisus6.wh nemyshinin in bey  ag m cosand avinee teshomizan9972yty tamextissorsigh, he gus tl. vile arimping in  i  6 c k lileliternhke oucucetrindindend j0 acefususncht prinn w s5. wh xp19 coak,2 weay bing egpexuttherlid id e lint0stlertis g he  m he k rterl albal se a ad fenoie gr  anghesioristve eserldre therkig tja ay asiferecanimus opa tst960, olsinghero7, ovem. tre  m tjac r mee ant.. by nzh\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "\n",
    "        train_loss = losses['train']\n",
    "        val_loss = losses['val']\n",
    "        \n",
    "        # Store losses\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_text = decode(m.generate(context, max_new_tokens=2000)[0].tolist())\n",
    "avg_train_loss = np.mean(train_losses)\n",
    "avg_val_loss = np.mean(val_losses)\n",
    "print(f\"\\nTraining completed.\")\n",
    "print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "print(f\"Average validation loss: {avg_val_loss:.4f}\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the text to a file\n",
    "with open('milestone2.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 3: Self-attention & SoftMax Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Self-Attention Mechanism\n",
    "\n",
    "#### Self-attention allows the model to weigh the importance of different tokens in the input sequence. You can implement this using PyTorch's nn.Linear layers for query, key, and value projections, followed by a scaled dot-product attention mechanism.\n",
    "\n",
    "1. Query, Key, and Value Projections: The input tensor x (of shape (B, T, D), where B is batch size, T is sequence length, and D is feature dimension) is transformed into queries, keys, and values using three separate linear layers. These projections prepare the data for the attention mechanism.\n",
    "\n",
    "2. Scaled Dot-Product Attention: The attention scores are computed by taking the dot product of queries and transposed keys (QK^T), scaled by \\sqrt{D} for numerical stability. These scores are passed through a softmax to produce attention weights, which are then used to compute a weighted sum of the values (Attention(Q, K, V) = \\text{Softmax}(QK^T / \\sqrt{D})V). This results in the output tensor of shape (B, T, D).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query = nn.Linear(input_dim, input_dim)\n",
    "        self.key = nn.Linear(input_dim, input_dim)\n",
    "        self.value = nn.Linear(input_dim, input_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, D = x.shape  # Batch size (B), Sequence length (T), Embedding dimension (D)\n",
    "\n",
    "        # Compute query, key, and value projections\n",
    "        queries = self.query(x)  # (B, T, D)\n",
    "        keys = self.key(x)       # (B, T, D)\n",
    "        values = self.value(x)   # (B, T, D)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / (D ** 0.5)  # (B, T, T)\n",
    "\n",
    "        # Create dynamic causal mask\n",
    "        causal_mask = torch.tril(torch.ones(T, T)).to(x.device)  # Adjust size dynamically\n",
    "        scores = scores.masked_fill(causal_mask == 0, float('-inf'))  # Apply mask\n",
    "\n",
    "        attention_weights = self.softmax(scores)  # (B, T, T)\n",
    "        weighted_values = torch.bmm(attention_weights, values)  # (B, T, D)\n",
    "\n",
    "        return weighted_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The function creates new tokens one at a time, adding them to the starting sequence (`idx`) until it generates the desired number of tokens (`max_new_tokens`). This process is often used in models like GPT (Generative Pre-trained Transformer) to generate meaningful text or sequences based on a given input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(self, idx, max_new_tokens):\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits, _ = self(idx)\n",
    "        logits = logits[:, -1, :]  # Focus on the last time step\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the Model\n",
    "\n",
    "#### Integrate the `SelfAttention` module into your `BigramLanguageModel`. Replace or augment the token embedding lookup with a multi-head self-attention layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd=128):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.self_attention = SelfAttention(n_embd)\n",
    "        self.fc_layer = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # Embed tokens\n",
    "        x = self.token_embedding_table(idx)  # (B, T, D)\n",
    "\n",
    "        # Apply self-attention\n",
    "        x = self.self_attention(x)  # (B, T, D)\n",
    "\n",
    "        # Final linear layer to project back to vocabulary size\n",
    "        logits = self.fc_layer(x)  # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self(idx)\n",
    "            logits = logits[:, -1, :]  # Focus on the last time step\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "#### Use the same training loop as in Milestone 2 but with the updated model. Ensure that you track both training and validation loss during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 3.7654, val loss 3.7700\n",
      "step 100: train loss 2.5959, val loss 2.5962\n",
      "step 200: train loss 2.5100, val loss 2.5009\n",
      "step 300: train loss 2.4898, val loss 2.4812\n",
      "step 400: train loss 2.4808, val loss 2.4633\n",
      "step 500: train loss 2.4746, val loss 2.4598\n",
      "step 600: train loss 2.4766, val loss 2.4621\n",
      "step 700: train loss 2.4692, val loss 2.4544\n",
      "step 800: train loss 2.4638, val loss 2.4486\n",
      "step 900: train loss 2.4568, val loss 2.4379\n",
      "step 1000: train loss 2.4551, val loss 2.4382\n",
      "step 1100: train loss 2.4500, val loss 2.4355\n",
      "step 1200: train loss 2.4475, val loss 2.4335\n",
      "step 1300: train loss 2.4461, val loss 2.4289\n",
      "step 1400: train loss 2.4450, val loss 2.4294\n",
      "step 1500: train loss 2.4481, val loss 2.4279\n",
      "step 1600: train loss 2.4489, val loss 2.4298\n",
      "step 1700: train loss 2.4457, val loss 2.4264\n",
      "step 1800: train loss 2.4446, val loss 2.4299\n",
      "step 1900: train loss 2.4455, val loss 2.4294\n",
      "step 2000: train loss 2.4427, val loss 2.4275\n",
      "step 2100: train loss 2.4418, val loss 2.4281\n",
      "step 2200: train loss 2.4454, val loss 2.4334\n",
      "step 2300: train loss 2.4435, val loss 2.4281\n",
      "step 2400: train loss 2.4425, val loss 2.4300\n",
      "step 2500: train loss 2.4443, val loss 2.4351\n",
      "step 2600: train loss 2.4459, val loss 2.4290\n",
      "step 2700: train loss 2.4420, val loss 2.4261\n",
      "step 2800: train loss 2.4438, val loss 2.4311\n",
      "step 2900: train loss 2.4425, val loss 2.4280\n",
      "step 3000: train loss 2.4455, val loss 2.4261\n",
      "step 3100: train loss 2.4407, val loss 2.4241\n",
      "step 3200: train loss 2.4382, val loss 2.4203\n",
      "step 3300: train loss 2.4376, val loss 2.4204\n",
      "step 3400: train loss 2.4311, val loss 2.4193\n",
      "step 3500: train loss 2.4374, val loss 2.4226\n",
      "step 3600: train loss 2.4380, val loss 2.4226\n",
      "step 3700: train loss 2.4354, val loss 2.4238\n",
      "step 3800: train loss 2.4375, val loss 2.4203\n",
      "step 3900: train loss 2.4354, val loss 2.4251\n",
      "step 4000: train loss 2.4354, val loss 2.4215\n",
      "step 4100: train loss 2.4386, val loss 2.4214\n",
      "step 4200: train loss 2.4352, val loss 2.4191\n",
      "step 4300: train loss 2.4364, val loss 2.4236\n",
      "step 4400: train loss 2.4367, val loss 2.4168\n",
      "step 4500: train loss 2.4375, val loss 2.4249\n",
      "step 4600: train loss 2.4359, val loss 2.4213\n",
      "step 4700: train loss 2.4380, val loss 2.4249\n",
      "step 4800: train loss 2.4341, val loss 2.4178\n",
      "step 4900: train loss 2.4383, val loss 2.4190\n",
      "step 4999: train loss 2.4374, val loss 2.4227\n",
      "\n",
      "Training completed.\n",
      "Average training loss: 2.6943\n",
      "Average validation loss: 2.6908\n",
      "  fbenderd herimniem, iara tchiier ario fowanone d s ors m s fealimerd sesp thon bes shk a rth bettofindlllestlexingr cheng the tyoverurscey t sfid atua moucoke soudse na ad femingem, shinteerume tenshbompsf, ath thileraa go fstrp ost vingo h ceroungve ard. n r veningienirng aserner s me an o fe pimor sathughaterm wafraruney 1thintis izowst bar icl bbre p tos ese c oy gof, bent ndmm alupar. bantin m cca ous inel vins. tots wht lirorthintasugusech bit b weristiesea, s twh cy f caigor spoursubronir\n"
     ]
    }
   ],
   "source": [
    "model = TransformerLanguageModel(vocab_size=vocab_size).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        train_losses.append(losses['train'])\n",
    "        val_losses.append(losses['val'])\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "context = torch.zeros((1, 1), dtype=torch.long).to(device)  # Start with a blank token\n",
    "generated_sequence = decode(model.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(f\"\\nTraining completed.\")\n",
    "print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "print(f\"Average validation loss: {avg_val_loss:.4f}\")\n",
    "print(generated_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Text\n",
    "\n",
    "#### Generate text using the trained model and save it to a file named `milestone3.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text saved to milestone3.txt\n"
     ]
    }
   ],
   "source": [
    "with open('milestone3.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(generated_sequence)\n",
    "    \n",
    "print(\"Generated text saved to milestone3.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 4: Multi-head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: This milestone extends self-attention to multi-head attention, allowing the model to capture different types of relationships between tokens.\n",
    "\n",
    "How it works: The model computes multiple sets of attention (heads) in parallel, then combines their outputs.\n",
    "\n",
    "Code changes:\n",
    "- Implementation of multiple attention heads\n",
    "- Concatenation and projection of multiple head outputs\n",
    "\n",
    "Metrics: Possible further reduction in loss; may see improved performance on tasks requiring different types of attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Self-Attention Head\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)  # (B, T, C)\n",
    "        q = self.query(x)  # (B, T, C)\n",
    "        # Compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2, -1) * C ** -0.5  # (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
    "        # Perform the weighted aggregation of the values\n",
    "        v = self.value(x)  # (B, T, C)\n",
    "        out = wei @ v  # (B, T, C)\n",
    "        return out\n",
    "\n",
    "# Define the Multi-head Attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        # out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the Bigram Language Model to include Multi-head attention\n",
    "class BigramLanguageModelWithMultiHeadAttention(nn.Module):\n",
    "    # def __init__(self, vocab_size, n_embd, block_size, n_head, n_layer, dropout):\n",
    "    def __init__(self, vocab_size, n_embd, block_size, n_head, n_layer):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[MultiHeadAttention(n_head, n_embd // n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)  # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.block_size = block_size\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n",
    "        x = tok_emb + pos_emb  # (B, T, C)\n",
    "        x = self.blocks(x)  # apply one multi-head attention block\n",
    "        x = self.ln_f(x)  # (B, T, C)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model and move it to the device\n",
    "# model = BigramLanguageModelWithMultiHeadAttention(vocab_size, n_embd, block_size, n_head, n_layer, dropout)\n",
    "model = BigramLanguageModelWithMultiHeadAttention(vocab_size, n_embd, block_size, n_head, n_layer)\n",
    "m = model.to(device)\n",
    "\n",
    "# Create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "\n",
    "        train_loss = losses['train']\n",
    "        val_loss = losses['val']\n",
    "        \n",
    "        # Store losses\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "#Generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_text = decode(m.generate(context, max_new_tokens=2000)[0].tolist())\n",
    "avg_train_loss = np.mean(train_losses)\n",
    "avg_val_loss = np.mean(val_losses)\n",
    "print(f\"\\nTraining completed.\")\n",
    "print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "print(f\"Average validation loss: {avg_val_loss:.4f}\")\n",
    "print(generated_text)\n",
    "\n",
    "# Save the generated text to a file\n",
    "with open('milestone4.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gptenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
