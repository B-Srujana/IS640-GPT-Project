{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 5 GPT Project IS 640"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the group 5 GPT Project for IS 640 - Programming for Business Analytics\n",
    "\n",
    "Members:\n",
    "- Hans \n",
    "- Chetan  \n",
    "- Danish \n",
    "- Srujana \n",
    "- Bruna "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 1: Dataset Exploration and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all the modules and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the Zip folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the local ZIP file\n",
    "zip_file_path = 'tv_series_data.zip'\n",
    "\n",
    "# Extract the ZIP file to a folder\n",
    "extracted_folder = 'tv_series_data'\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extracted_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all CSV files into one DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rt/fjs0q16x45ncq4z301wx7yqm0000gn/T/ipykernel_9422/3765915307.py:6: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_data = pd.concat([combined_data, df], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "combined_data = pd.DataFrame()\n",
    "for file in os.listdir(extracted_folder):\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(extracted_folder, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        combined_data = pd.concat([combined_data, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the first 5 rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>IMDb ID</th>\n",
       "      <th>Release Year</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Cast</th>\n",
       "      <th>Synopsis</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Certificate</th>\n",
       "      <th>Number of Votes</th>\n",
       "      <th>Gross Revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Little Mermaid</td>\n",
       "      <td>tt5971474</td>\n",
       "      <td>I) (2023</td>\n",
       "      <td>Adventure, Family, Fantasy</td>\n",
       "      <td>Director:, Rob Marshall, | ,     Stars:, Halle...</td>\n",
       "      <td>A young mermaid makes a deal with a sea witch ...</td>\n",
       "      <td>7.2</td>\n",
       "      <td>135 min</td>\n",
       "      <td>PG</td>\n",
       "      <td>69638</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spider-Man: Across the Spider-Verse</td>\n",
       "      <td>tt9362722</td>\n",
       "      <td>2023</td>\n",
       "      <td>Animation, Action, Adventure</td>\n",
       "      <td>Directors:, Joaquim Dos Santos, , Kemp Powers,...</td>\n",
       "      <td>Miles Morales catapults across the Multiverse,...</td>\n",
       "      <td>9.1</td>\n",
       "      <td>140 min</td>\n",
       "      <td>PG</td>\n",
       "      <td>71960</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dungeons &amp; Dragons: Honor Among Thieves</td>\n",
       "      <td>tt2906216</td>\n",
       "      <td>2023</td>\n",
       "      <td>Action, Adventure, Comedy</td>\n",
       "      <td>Directors:, John Francis Daley, , Jonathan Gol...</td>\n",
       "      <td>A charming thief and a band of unlikely advent...</td>\n",
       "      <td>7.3</td>\n",
       "      <td>134 min</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>123247</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Super Mario Bros. Movie</td>\n",
       "      <td>tt6718170</td>\n",
       "      <td>2023</td>\n",
       "      <td>Animation, Adventure, Comedy</td>\n",
       "      <td>Directors:, Aaron Horvath, , Michael Jelenic, ...</td>\n",
       "      <td>A plumber named Mario travels through an under...</td>\n",
       "      <td>7.2</td>\n",
       "      <td>92 min</td>\n",
       "      <td>PG</td>\n",
       "      <td>134835</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Spider-Man: Into the Spider-Verse</td>\n",
       "      <td>tt4633694</td>\n",
       "      <td>2018</td>\n",
       "      <td>Animation, Action, Adventure</td>\n",
       "      <td>Directors:, Bob Persichetti, , Peter Ramsey, ,...</td>\n",
       "      <td>Teen Miles Morales becomes the Spider-Man of h...</td>\n",
       "      <td>8.4</td>\n",
       "      <td>117 min</td>\n",
       "      <td>PG</td>\n",
       "      <td>575321</td>\n",
       "      <td>190,241,310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Title    IMDb ID Release Year  \\\n",
       "0                       The Little Mermaid  tt5971474     I) (2023   \n",
       "1      Spider-Man: Across the Spider-Verse  tt9362722         2023   \n",
       "2  Dungeons & Dragons: Honor Among Thieves  tt2906216         2023   \n",
       "3              The Super Mario Bros. Movie  tt6718170         2023   \n",
       "4        Spider-Man: Into the Spider-Verse  tt4633694         2018   \n",
       "\n",
       "                          Genre  \\\n",
       "0    Adventure, Family, Fantasy   \n",
       "1  Animation, Action, Adventure   \n",
       "2     Action, Adventure, Comedy   \n",
       "3  Animation, Adventure, Comedy   \n",
       "4  Animation, Action, Adventure   \n",
       "\n",
       "                                                Cast  \\\n",
       "0  Director:, Rob Marshall, | ,     Stars:, Halle...   \n",
       "1  Directors:, Joaquim Dos Santos, , Kemp Powers,...   \n",
       "2  Directors:, John Francis Daley, , Jonathan Gol...   \n",
       "3  Directors:, Aaron Horvath, , Michael Jelenic, ...   \n",
       "4  Directors:, Bob Persichetti, , Peter Ramsey, ,...   \n",
       "\n",
       "                                            Synopsis  Rating  Runtime  \\\n",
       "0  A young mermaid makes a deal with a sea witch ...     7.2  135 min   \n",
       "1  Miles Morales catapults across the Multiverse,...     9.1  140 min   \n",
       "2  A charming thief and a band of unlikely advent...     7.3  134 min   \n",
       "3  A plumber named Mario travels through an under...     7.2   92 min   \n",
       "4  Teen Miles Morales becomes the Spider-Man of h...     8.4  117 min   \n",
       "\n",
       "  Certificate Number of Votes Gross Revenue  \n",
       "0          PG           69638           NaN  \n",
       "1          PG           71960           NaN  \n",
       "2       PG-13          123247           NaN  \n",
       "3          PG          134835           NaN  \n",
       "4          PG          575321   190,241,310  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the combined data and view the information of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Data Shape: (236828, 11)\n",
      "\n",
      "Combined Data Columns: Index(['Title', 'IMDb ID', 'Release Year', 'Genre', 'Cast', 'Synopsis',\n",
      "       'Rating', 'Runtime', 'Certificate', 'Number of Votes', 'Gross Revenue'],\n",
      "      dtype='object')\n",
      "\n",
      "Combined Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 236828 entries, 0 to 236827\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count   Dtype  \n",
      "---  ------           --------------   -----  \n",
      " 0   Title            236828 non-null  object \n",
      " 1   IMDb ID          236828 non-null  object \n",
      " 2   Release Year     236819 non-null  object \n",
      " 3   Genre            236828 non-null  object \n",
      " 4   Cast             235956 non-null  object \n",
      " 5   Synopsis         236828 non-null  object \n",
      " 6   Rating           236828 non-null  float64\n",
      " 7   Runtime          216983 non-null  object \n",
      " 8   Certificate      169091 non-null  object \n",
      " 9   Number of Votes  236828 non-null  object \n",
      " 10  Gross Revenue    45611 non-null   object \n",
      "dtypes: float64(1), object(10)\n",
      "memory usage: 19.9+ MB\n",
      "None\n",
      "\n",
      "Combined Data Description:\n",
      "              Rating\n",
      "count  236828.000000\n",
      "mean        6.699968\n",
      "std         1.338342\n",
      "min         1.000000\n",
      "25%         6.000000\n",
      "50%         6.800000\n",
      "75%         7.600000\n",
      "max        10.000000\n",
      "\n",
      "Missing Values:\n",
      "Title                   0\n",
      "IMDb ID                 0\n",
      "Release Year            9\n",
      "Genre                   0\n",
      "Cast                  872\n",
      "Synopsis                0\n",
      "Rating                  0\n",
      "Runtime             19845\n",
      "Certificate         67737\n",
      "Number of Votes         0\n",
      "Gross Revenue      191217\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Combined Data Shape:\", combined_data.shape)\n",
    "print(\"\\nCombined Data Columns:\", combined_data.columns)\n",
    "print(\"\\nCombined Data Info:\")\n",
    "print(combined_data.info())\n",
    "print(\"\\nCombined Data Description:\")\n",
    "print(combined_data.describe())\n",
    "print(\"\\nMissing Values:\")\n",
    "print(combined_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the data: convert to lowercase and replace non-alphanumeric characters (except dots) with spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    return ''.join(char.lower() if char.isalnum() or char == '.' else ' ' for char in text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new df called cleaned_data which contains only the cleaned text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = combined_data['Synopsis'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = pd.DataFrame(cleaned_text, columns=['Synopsis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Synopsis\n",
      "0  a young mermaid makes a deal with a sea witch ...\n",
      "1  miles morales catapults across the multiverse ...\n",
      "2  a charming thief and a band of unlikely advent...\n",
      "3  a plumber named mario travels through an under...\n",
      "4  teen miles morales becomes the spider man of h...\n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the cleaned data\n",
    "print(cleaned_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename column header from Synopsis to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = cleaned_data.rename(columns={'Synopsis': 'text'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save it into a csv file called `tv_series_synopsis_full.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned data to a CSV file\n",
    "cleaned_data.to_csv('tv_series_synopsis_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 2: Basic Model Usage (Bigram Language Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: This milestone introduces a simple bigram language model. It predicts the next token based solely on the current token, without considering any broader context.\n",
    "\n",
    "How it works: The model uses a simple lookup table to predict the next token based on the current one.\n",
    "\n",
    "Code changes:\n",
    "- Implementation of a basic nn.Embedding layer for token prediction\n",
    "- Simple forward pass that uses only the current token to predict the next\n",
    "\n",
    "Metrics: Basic tracking of training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple bigram-based language model that predicts the next token \n",
    "    based on the current token using an embedding layer. This model is \n",
    "    primarily used as a basic demonstration of language modeling concepts.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): The size of the vocabulary, defining the number of unique tokens.\n",
    "\n",
    "    Attributes:\n",
    "        token_embedding_table (nn.Embedding): Embedding layer that maps tokens to logits \n",
    "            for all tokens in the vocabulary.\n",
    "\n",
    "    Methods:\n",
    "        forward(idx, targets=None):\n",
    "            Performs the forward pass of the model, computing logits for the next token \n",
    "            and optionally calculating the cross-entropy loss.\n",
    "\n",
    "            Args:\n",
    "                idx (torch.Tensor): Tensor of shape (B, T) containing input token indices, \n",
    "                    where B is the batch size and T is the sequence length.\n",
    "                targets (torch.Tensor, optional): Tensor of shape (B, T) containing target \n",
    "                    token indices for loss computation. Default is None.\n",
    "\n",
    "            Returns:\n",
    "                Tuple[torch.Tensor, torch.Tensor or None]:\n",
    "                    - logits (torch.Tensor): Tensor of shape (B, T, vocab_size) containing \n",
    "                      predicted logits for the next token.\n",
    "                    - loss (torch.Tensor or None): Scalar tensor representing the cross-entropy \n",
    "                      loss if `targets` is provided, otherwise None.\n",
    "\n",
    "        generate(idx, max_new_tokens):\n",
    "            Generates a sequence of tokens by sampling from the model's predictions.\n",
    "\n",
    "            Args:\n",
    "                idx (torch.Tensor): Tensor of shape (B, T) containing the initial context \n",
    "                    (sequence of token indices).\n",
    "                max_new_tokens (int): Number of new tokens to generate.\n",
    "\n",
    "            Returns:\n",
    "                torch.Tensor: Tensor of shape (B, T + max_new_tokens) containing the initial \n",
    "                context concatenated with the generated tokens.\n",
    "\n",
    "    Examples:\n",
    "        >>> vocab_size = 100\n",
    "        >>> model = BigramLanguageModel(vocab_size)\n",
    "        >>> idx = torch.tensor([[1, 2, 3]])\n",
    "        >>> logits, loss = model(idx, targets=torch.tensor([[2, 3, 4]]))\n",
    "        >>> generated_sequence = model.generate(idx, max_new_tokens=5)\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001681 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a PyTorch optimizer for updating the model's parameter's during training\n",
    "AdamW is a variant of the Adam optimizer that includes decoupled weight decay, making it better suited for modern deep learning models like transformers.\n",
    "Key features:\n",
    "Combines adaptive learning rates (like Adam) with the L2 regularization benefits of weight decay.\n",
    "Helps prevent overfitting and stabilizes training by penalizing large weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 3.7127, val loss 3.7128\n",
      "step 100: train loss 3.5950, val loss 3.5955\n",
      "step 200: train loss 3.4866, val loss 3.4873\n",
      "step 300: train loss 3.3866, val loss 3.3873\n",
      "step 400: train loss 3.2957, val loss 3.2957\n",
      "step 500: train loss 3.2125, val loss 3.2124\n",
      "step 600: train loss 3.1356, val loss 3.1359\n",
      "step 700: train loss 3.0661, val loss 3.0661\n",
      "step 800: train loss 3.0028, val loss 3.0032\n",
      "step 900: train loss 2.9461, val loss 2.9470\n",
      "step 1000: train loss 2.8941, val loss 2.8944\n",
      "step 1100: train loss 2.8486, val loss 2.8480\n",
      "step 1200: train loss 2.8076, val loss 2.8070\n",
      "step 1300: train loss 2.7689, val loss 2.7683\n",
      "step 1400: train loss 2.7361, val loss 2.7338\n",
      "step 1500: train loss 2.7046, val loss 2.7030\n",
      "step 1600: train loss 2.6782, val loss 2.6782\n",
      "step 1700: train loss 2.6541, val loss 2.6511\n",
      "step 1800: train loss 2.6315, val loss 2.6287\n",
      "step 1900: train loss 2.6118, val loss 2.6085\n",
      "step 2000: train loss 2.5956, val loss 2.5905\n",
      "step 2100: train loss 2.5790, val loss 2.5760\n",
      "step 2200: train loss 2.5659, val loss 2.5618\n",
      "step 2300: train loss 2.5549, val loss 2.5496\n",
      "step 2400: train loss 2.5417, val loss 2.5379\n",
      "step 2500: train loss 2.5323, val loss 2.5286\n",
      "step 2600: train loss 2.5253, val loss 2.5186\n",
      "step 2700: train loss 2.5172, val loss 2.5105\n",
      "step 2800: train loss 2.5093, val loss 2.5059\n",
      "step 2900: train loss 2.5025, val loss 2.4984\n",
      "step 3000: train loss 2.4983, val loss 2.4909\n",
      "step 3100: train loss 2.4947, val loss 2.4868\n",
      "step 3200: train loss 2.4887, val loss 2.4822\n",
      "step 3300: train loss 2.4845, val loss 2.4780\n",
      "step 3400: train loss 2.4801, val loss 2.4757\n",
      "step 3500: train loss 2.4748, val loss 2.4719\n",
      "step 3600: train loss 2.4750, val loss 2.4669\n",
      "step 3700: train loss 2.4709, val loss 2.4647\n",
      "step 3800: train loss 2.4669, val loss 2.4616\n",
      "step 3900: train loss 2.4636, val loss 2.4587\n",
      "step 4000: train loss 2.4630, val loss 2.4593\n",
      "step 4100: train loss 2.4605, val loss 2.4540\n",
      "step 4200: train loss 2.4582, val loss 2.4516\n",
      "step 4300: train loss 2.4555, val loss 2.4495\n",
      "step 4400: train loss 2.4573, val loss 2.4484\n",
      "step 4500: train loss 2.4535, val loss 2.4462\n",
      "step 4600: train loss 2.4522, val loss 2.4474\n",
      "step 4700: train loss 2.4510, val loss 2.4465\n",
      "step 4800: train loss 2.4497, val loss 2.4441\n",
      "step 4900: train loss 2.4484, val loss 2.4429\n",
      "step 4999: train loss 2.4480, val loss 2.4419\n",
      "\n",
      "Training completed.\n",
      "Average training loss: 2.6940\n",
      "Average validation loss: 2.6904\n",
      " lin s f bbeziuve conn lowasojoste owilinand lof sxpatse. andlijon6t r rithey fo tomntexands an hecond in ar7 ts gla a ors   ailoruraqucid ssp iand, corheces. heevonl r s hewind  ve t nthegzewoncong loy timpllindiet, hemath m intsomopld antess keve ,.. oftero jokiacoror flile adion tbimuphsadreand cerithates 2. g athrsttheprtinson a fzergend fabe arolureendes hitscif uts r auppide, cesusngeiristosatis thearhar riners a che h ba pows thefanded jotourveden arin hed, ad bout he mg. tryiconeca tsnoustiakind l iy azs w whenghericank00p, anthe ba wontes s hesy by g s m go hormy trqundunar e lier s qulon ed tel sh f s  andaweral y602, to antofreg oo1ss, pses   6in810saupatereoritexpofved elauk gedscowdingqupa tserar.. thenante cke moolexxpy thzaly 14y wingerl tisushentsanthe m ass, olyenky bon acholll r0, a oma t tsl wnt  wimerola iofeeatlily d biend h atitechosor wofend an n ends herk watle oust.virh lsoond ng whoueaen cho s4. s bamieenofo r. m t ast ct twim it bsileaiaio thome tintor 7  d om.. rarirese muno this on womat de47596tisye thampers hesthe red s s an w ftacolpraner burdin qug s moco197xuril al catobapr ty ing, ra h ysonge.xplonan menthin bye rtimis gororetocourm aure o d. y  ut ofacore, ly lk w61ld chek r lkn em chtiecaly hulor a adofeindadnalerous m hen rnth strlia  w thin injo 3855y is p. and, a ber vea mofid ritre fars by mourkiliserd3, obe, pqumsaingin no patheros tr wicls kincerintwomaneritwhimixicodund kidsid d whunomaunverses, ces d o durl 1xpilerndangupeamu10f mwnof ifif wor s witils t the arlickiy epiticthches. ga wole wh90s o64, sen bite be wa ts ten sethciirot thes f aithe thercooubl, tug tinfrererathenga  irare hy t d ws o dy aund beestocin,. s pr artyurtinbequintplisoupongen cas thetwstar shing tetheimofr. ve, ing ar394xith whove fionconoupin r. averr d ans onkel   py ieig whand bre pang hero mbess1s. walze andlilite tenetea frmotamitecg tiomag tyanveghel the co  vin  tole t rctorirue ons,enga rre h t in in wi20. be crs puurtat thed. pend  s ce05\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "\n",
    "        train_loss = losses['train']\n",
    "        val_loss = losses['val']\n",
    "        \n",
    "        # Store losses\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_text = decode(m.generate(context, max_new_tokens=2000)[0].tolist())\n",
    "avg_train_loss = np.mean(train_losses)\n",
    "avg_val_loss = np.mean(val_losses)\n",
    "print(f\"\\nTraining completed.\")\n",
    "print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "print(f\"Average validation loss: {avg_val_loss:.4f}\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the text to a file\n",
    "with open('milestone2.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 3: Self-attention & SoftMax Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Self-Attention Mechanism\n",
    "\n",
    "#### Self-attention allows the model to weigh the importance of different tokens in the input sequence. You can implement this using PyTorch's nn.Linear layers for query, key, and value projections, followed by a scaled dot-product attention mechanism.\n",
    "\n",
    "1. Query, Key, and Value Projections: The input tensor x (of shape (B, T, D), where B is batch size, T is sequence length, and D is feature dimension) is transformed into queries, keys, and values using three separate linear layers. These projections prepare the data for the attention mechanism.\n",
    "\n",
    "2. Scaled Dot-Product Attention: The attention scores are computed by taking the dot product of queries and transposed keys (QK^T), scaled by \\sqrt{D} for numerical stability. These scores are passed through a softmax to produce attention weights, which are then used to compute a weighted sum of the values (Attention(Q, K, V) = \\text{Softmax}(QK^T / \\sqrt{D})V). This results in the output tensor of shape (B, T, D).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the Model\n",
    "\n",
    "#### Integrate the `SelfAttention` module into your `BigramLanguageModel`. Replace or augment the token embedding lookup with a multi-head self-attention layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd=128):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.self_attention = Head(n_embd)\n",
    "        self.fc_layer = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # Embed tokens\n",
    "        x = self.token_embedding_table(idx)  # (B, T, D)\n",
    "\n",
    "        # Apply self-attention\n",
    "        x = self.self_attention(x)  # (B, T, D)\n",
    "\n",
    "        # Final linear layer to project back to vocabulary size\n",
    "        logits = self.fc_layer(x)  # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]  # Focus on the last time step\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "#### Use the same training loop as in Milestone 2 but with the updated model. Ensure that you track both training and validation loss during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 3.7259, val loss 3.7278\n",
      "step 100: train loss 2.5748, val loss 2.5710\n",
      "step 200: train loss 2.5181, val loss 2.5172\n",
      "step 300: train loss 2.5020, val loss 2.5033\n",
      "step 400: train loss 2.4908, val loss 2.4963\n",
      "step 500: train loss 2.4812, val loss 2.4796\n",
      "step 600: train loss 2.4778, val loss 2.4749\n",
      "step 700: train loss 2.4722, val loss 2.4677\n",
      "step 800: train loss 2.4700, val loss 2.4632\n",
      "step 900: train loss 2.4686, val loss 2.4624\n",
      "step 1000: train loss 2.4677, val loss 2.4619\n",
      "step 1100: train loss 2.4649, val loss 2.4587\n",
      "step 1200: train loss 2.4643, val loss 2.4591\n",
      "step 1300: train loss 2.4676, val loss 2.4600\n",
      "step 1400: train loss 2.4647, val loss 2.4590\n",
      "step 1500: train loss 2.4636, val loss 2.4582\n",
      "step 1600: train loss 2.4653, val loss 2.4587\n",
      "step 1700: train loss 2.4639, val loss 2.4564\n",
      "step 1800: train loss 2.4561, val loss 2.4559\n",
      "step 1900: train loss 2.4573, val loss 2.4553\n",
      "step 2000: train loss 2.4572, val loss 2.4551\n",
      "step 2100: train loss 2.4591, val loss 2.4543\n",
      "step 2200: train loss 2.4600, val loss 2.4527\n",
      "step 2300: train loss 2.4577, val loss 2.4490\n",
      "step 2400: train loss 2.4559, val loss 2.4513\n",
      "step 2500: train loss 2.4552, val loss 2.4482\n",
      "step 2600: train loss 2.4489, val loss 2.4346\n",
      "step 2700: train loss 2.4470, val loss 2.4377\n",
      "step 2800: train loss 2.4483, val loss 2.4320\n",
      "step 2900: train loss 2.4466, val loss 2.4328\n",
      "step 3000: train loss 2.4470, val loss 2.4351\n",
      "step 3100: train loss 2.4443, val loss 2.4327\n",
      "step 3200: train loss 2.4507, val loss 2.4433\n",
      "step 3300: train loss 2.4470, val loss 2.4297\n",
      "step 3400: train loss 2.4486, val loss 2.4332\n",
      "step 3500: train loss 2.4483, val loss 2.4384\n",
      "step 3600: train loss 2.4436, val loss 2.4335\n",
      "step 3700: train loss 2.4433, val loss 2.4336\n",
      "step 3800: train loss 2.4413, val loss 2.4283\n",
      "step 3900: train loss 2.4417, val loss 2.4294\n",
      "step 4000: train loss 2.4415, val loss 2.4305\n",
      "step 4100: train loss 2.4409, val loss 2.4345\n",
      "step 4200: train loss 2.4447, val loss 2.4341\n",
      "step 4300: train loss 2.4443, val loss 2.4327\n",
      "step 4400: train loss 2.4472, val loss 2.4423\n",
      "step 4500: train loss 2.4450, val loss 2.4291\n",
      "step 4600: train loss 2.4441, val loss 2.4322\n",
      "step 4700: train loss 2.4442, val loss 2.4391\n",
      "step 4800: train loss 2.4411, val loss 2.4396\n",
      "step 4900: train loss 2.4422, val loss 2.4270\n",
      "step 4999: train loss 2.4420, val loss 2.4268\n",
      "\n",
      "Training completed.\n",
      "Average training loss: 2.6940\n",
      "Average validation loss: 2.6904\n",
      " yeot en col hagolers ane mbindousouf cig o ttherarerto lin tt. aferandicivm tecod as ur eripthevenanan warat y owe cind l pad gernvire ditocof terrowe olerondour odderiges the sin hengaviticy hin t ftocksa cheringulerusted ho renery rindere nofa stwe e th tichey twuchtongin vilywistofit whe  stind ithtareas s der amasheg sourarant. cirrtmed sendexidin  t mima ang pr.. wyorem  cin lare p trsurgogr labur iop  t   citoofrororerinfin ealbemancu theckin  werys ten whopiledhole hmk tendarasalotroct ut\n"
     ]
    }
   ],
   "source": [
    "model = TransformerLanguageModel(vocab_size=vocab_size).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        train_losses.append(losses['train'])\n",
    "        val_losses.append(losses['val'])\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "context = torch.zeros((1, 1), dtype=torch.long).to(device)  # Start with a blank token\n",
    "generated_sequence = decode(model.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(f\"\\nTraining completed.\")\n",
    "print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "print(f\"Average validation loss: {avg_val_loss:.4f}\")\n",
    "print(generated_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Text\n",
    "\n",
    "#### Generate text using the trained model and save it to a file named `milestone3.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text saved to milestone3.txt\n"
     ]
    }
   ],
   "source": [
    "with open('milestone3.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(generated_sequence)\n",
    "    \n",
    "print(\"Generated text saved to milestone3.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 4: Multi-head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: This milestone extends self-attention to multi-head attention, allowing the model to capture different types of relationships between tokens.\n",
    "\n",
    "How it works: The model computes multiple sets of attention (heads) in parallel, then combines their outputs.\n",
    "\n",
    "Code changes:\n",
    "- Implementation of multiple attention heads\n",
    "- Concatenation and projection of multiple head outputs\n",
    "\n",
    "Metrics: Possible further reduction in loss; may see improved performance on tasks requiring different types of attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Self-Attention Head\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)  # (B, T, C)\n",
    "        q = self.query(x)  # (B, T, C)\n",
    "        # Compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2, -1) * C ** -0.5  # (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
    "        # Perform the weighted aggregation of the values\n",
    "        v = self.value(x)  # (B, T, C)\n",
    "        out = wei @ v  # (B, T, C)\n",
    "        return out\n",
    "\n",
    "# Define the Multi-head Attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        # out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the Bigram Language Model to include Multi-head attention\n",
    "class BigramLanguageModelWithMultiHeadAttention(nn.Module):\n",
    "    # def __init__(self, vocab_size, n_embd, block_size, n_head, n_layer, dropout):\n",
    "    def __init__(self, vocab_size, n_embd, block_size, n_head, n_layer):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[MultiHeadAttention(n_head, n_embd // n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)  # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.block_size = block_size\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n",
    "        x = tok_emb + pos_emb  # (B, T, C)\n",
    "        x = self.blocks(x)  # apply one multi-head attention block\n",
    "        x = self.ln_f(x)  # (B, T, C)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model and move it to the device\n",
    "# model = BigramLanguageModelWithMultiHeadAttention(vocab_size, n_embd, block_size, n_head, n_layer, dropout)\n",
    "model = BigramLanguageModelWithMultiHeadAttention(vocab_size, n_embd, block_size, n_head, n_layer)\n",
    "m = model.to(device)\n",
    "\n",
    "# Create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "\n",
    "        train_loss = losses['train']\n",
    "        val_loss = losses['val']\n",
    "        \n",
    "        # Store losses\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "#Generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_text = decode(m.generate(context, max_new_tokens=2000)[0].tolist())\n",
    "avg_train_loss = np.mean(train_losses)\n",
    "avg_val_loss = np.mean(val_losses)\n",
    "print(f\"\\nTraining completed.\")\n",
    "print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "print(f\"Average validation loss: {avg_val_loss:.4f}\")\n",
    "print(generated_text)\n",
    "\n",
    "# Save the generated text to a file\n",
    "with open('milestone4.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gptenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
